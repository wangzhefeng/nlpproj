{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a4d67d-73c3-46dc-95af-8d9b641e8e32",
   "metadata": {},
   "source": [
    "# 核心概念\n",
    "\n",
    "1. document\n",
    "2. corpus\n",
    "3. vector\n",
    "4. model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e3566-4707-4406-8538-44458f6f53bb",
   "metadata": {},
   "source": [
    "## document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e924442e-080f-431e-b27c-3ed7e529b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a673-5c6e-4bda-9c99-c4aa78703370",
   "metadata": {},
   "source": [
    "## corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e1dbfc7-f5dd-4b3e-884f-b3b21a23417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaaab2b-8802-48f4-8e47-577cad8c41a4",
   "metadata": {},
   "source": [
    "### stop words filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f7fc54-31e2-4b9d-8a49-5b04e224e918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set of frequent words\n",
    "stoplist = set(\"for a of the and to in\".split(\" \"))\n",
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76aa4895-1930-42e7-8c57-fa75c4d1eaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase each document, split it by whitespace and filter out stopwords\n",
    "texts = [\n",
    "    [\n",
    "        word for word in document.lower().split() \n",
    "        if word not in stoplist\n",
    "    ]\n",
    "    for document in text_corpus\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc902c10-101f-484d-a0ac-83695a5355a3",
   "metadata": {},
   "source": [
    "### word frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1e3ee3e-a31a-4e7d-b249-b9cbe27394c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'human': 2,\n",
       "             'machine': 1,\n",
       "             'interface': 2,\n",
       "             'lab': 1,\n",
       "             'abc': 1,\n",
       "             'computer': 2,\n",
       "             'applications': 1,\n",
       "             'survey': 2,\n",
       "             'user': 3,\n",
       "             'opinion': 1,\n",
       "             'system': 4,\n",
       "             'response': 2,\n",
       "             'time': 2,\n",
       "             'eps': 2,\n",
       "             'management': 1,\n",
       "             'engineering': 1,\n",
       "             'testing': 1,\n",
       "             'relation': 1,\n",
       "             'perceived': 1,\n",
       "             'error': 1,\n",
       "             'measurement': 1,\n",
       "             'generation': 1,\n",
       "             'random': 1,\n",
       "             'binary': 1,\n",
       "             'unordered': 1,\n",
       "             'trees': 3,\n",
       "             'intersection': 1,\n",
       "             'graph': 3,\n",
       "             'paths': 1,\n",
       "             'minors': 2,\n",
       "             'iv': 1,\n",
       "             'widths': 1,\n",
       "             'well': 1,\n",
       "             'quasi': 1,\n",
       "             'ordering': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5894b1c-d284-4b03-8ffc-8e2b406f7143",
   "metadata": {},
   "source": [
    "### word frequency filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c22ee649-3e2f-4221-a9dc-3b556fc57e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep words that appear more than once\n",
    "processed_corpus = [\n",
    "    [\n",
    "        token for token in text \n",
    "        if frequency[token] > 1\n",
    "    ]\n",
    "    for text in texts\n",
    "]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782af133-3d07-44a8-aaf1-a6cf450ed049",
   "metadata": {},
   "source": [
    "### dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11ca09fd-ba07-47ed-bd61-a233d3ad63df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "# dictionary.save(\"./saved_results/deerwester.dict\")\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfb1a8-8706-4c40-bdf5-e67840c03076",
   "metadata": {},
   "source": [
    "## vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87295987-4af3-48a0-b971-820c1e5c8042",
   "metadata": {},
   "source": [
    "#### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d83526d-db1c-4c84-a0de-e129e240d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(dictonary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba641e6-aa9e-4575-8015-90d775dfb9cb",
   "metadata": {},
   "source": [
    "#### bag-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e0812ba-2b8e-42c5-be7f-85fdff96d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'computer', 'interaction']\n",
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\".lower().split()\n",
    "print(new_doc)\n",
    "\n",
    "new_vec = dictionary.doc2bow(new_doc)\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e23b88f5-7331-444d-831b-9a2ce01a0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# corpora.MmCorpus.serialize(\"./saved_results/deerwester.mm\", bow_corpus)\n",
    "# corpora.SvmLightCorpus.serialize(\"./saved_results/deerwester.svmlight\", bow_corpus)\n",
    "# corpora.BleiCorpus.serialize(\"./saved_results/deerwester.lda-c\", bow_corpus)\n",
    "# corpora.LowCorpus.serialize(\"./saved_results/deerwester.low\", bow_corpus)\n",
    "\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb94485-e35a-468b-942f-bc6418c3f010",
   "metadata": {},
   "source": [
    "## corpus 和 vector Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b3997c7-3e6f-46fc-a5ba-67b2a7c9743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x30101de90>\n",
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n",
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "\n",
    "# 语料\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open(\"https://radimrehurek.com/mycorpus.txt\"):\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "\n",
    "corpus_memory_friendly = MyCorpus()\n",
    "print(corpus_memory_friendly)\n",
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)\n",
    "\n",
    "\n",
    "# 字典\n",
    "dictionary = corpora.Dictionary(\n",
    "    line.lower().split() \n",
    "    for line in open(\"https://radimrehurek.com/mycorpus.txt\")\n",
    ")\n",
    "# 停止词\n",
    "stoplist = set(\"for a of the and to in\".split(\" \"))\n",
    "stop_ids = [\n",
    "    dictionary.token2id[stopword]\n",
    "    for stopword in stoplist\n",
    "    if stopword in dictionary.token2id\n",
    "]\n",
    "# 频率统计\n",
    "once_ids = [\n",
    "    token_id \n",
    "    for token_id, doc_freq in dictionary.dfs.items() \n",
    "    if doc_freq == 1\n",
    "]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208336c2-34fe-4d8a-aa58-0a8e9f54b036",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ff85b-5b6c-4d12-938e-ac17930d9f51",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e26332e-e994-42aa-a3e9-2b18c1461e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train model\n",
    "tf_idf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00bfc2e4-6151-469d-86ce-20ba3cf90b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n"
     ]
    }
   ],
   "source": [
    "# transform the 'system minors' string\n",
    "words = \"system minors\".lower().split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db547fed-81fb-4a7f-9f8d-1cc820d20cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "res = tf_idf[dictionary.doc2bow(words)]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c840180-4180-4bc2-b112-b4a0e923c61f",
   "metadata": {},
   "source": [
    "### 相似性查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3153e70-a447-4063-b083-d811a7ebb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.similarities.docsim.SparseMatrixSimilarity object at 0x300a49710>\n",
      "['system', 'engineering']\n",
      "[(5, 1)]\n",
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.SparseMatrixSimilarity(tf_idf[bow_corpus], num_features=12)\n",
    "print(index)\n",
    "\n",
    "query_document = \"system engineering\".lower().split()\n",
    "print(query_document)\n",
    "\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "print(query_bow)\n",
    "\n",
    "sims = index[tf_idf[query_bow]]\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd0367-7e95-4e23-8eea-deb79397d5aa",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b785c358-b465-430e-8190-c616c8e16988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 30.1% 500.0/1662.8MB downloaded"
     ]
    },
    {
     "ename": "ContentTooShortError",
     "evalue": "<urlopen error retrieval incomplete: got only 524288000 out of 1743563840 bytes>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContentTooShortError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m wv \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vec-google-news-300\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/llm/lib/python3.11/site-packages/gensim/downloader.py:496\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    494\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, file_name)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(folder_dir):\n\u001b[0;32m--> 496\u001b[0m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/llm/lib/python3.11/site-packages/gensim/downloader.py:396\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    394\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{fname}\u001b[39;00m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fname\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    395\u001b[0m dst_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmp_dir, fname)\n\u001b[0;32m--> 396\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _calculate_md5_checksum(dst_path) \u001b[38;5;241m==\u001b[39m _get_checksum(name):\n\u001b[1;32m    398\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/llm/lib/python3.11/urllib/request.py:280\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 reporthook(blocknum, bs, size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m read \u001b[38;5;241m<\u001b[39m size:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentTooShortError(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval incomplete: got only \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m%\u001b[39m (read, size), result)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mContentTooShortError\u001b[0m: <urlopen error retrieval incomplete: got only 524288000 out of 1743563840 bytes>"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd622eb-59e0-43d1-b643-fb0ed041066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b5e20-be68-4ae2-ad99-bfe654755dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv[\"king\"]\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcbbac-3c31-4aae-9235-e2ff4dc95b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vec_cameroon = wv[\"cameroon\"]\n",
    "except KeyError:\n",
    "    print(f\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63857138-5506-41e5-88a2-fb29a4047971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 内置支持多种词语相似性任务\n",
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b1838-2b3f-4b34-9af9-8f3a53b1a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印与“car”或“minivan”最相似的 5 个词\n",
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd29554-86d9-4808-b522-e5038b386472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下哪个不属于这个序列？\n",
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f034c6e-e86b-45f2-aec5-bce5e52def68",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773d81b-a349-49d0-a49c-45d6a4144e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
